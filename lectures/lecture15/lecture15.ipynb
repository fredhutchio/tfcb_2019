{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectures 15 and 16: Data analysis with `pandas` and `scikit-learn`\n",
    "\n",
    "One of the strengths of the python language is the amazing collection of packages built on top of the core library. Here we'll learn about two libraries that are useful for \"data science\". \n",
    "\n",
    "`pandas` is a multi-purpose data analysis library built on top of numpy and matplotlib. It's great for analyzing tabular data.\n",
    "\n",
    "https://pandas.pydata.org/\n",
    "\n",
    "Some useful introductory `pandas` docs:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/getting_started/basics.html\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/cookbook.html\n",
    "\n",
    "\n",
    "`scikit-learn` is a nice library of machine learning tools with a clean and consistent interface. \n",
    "\n",
    "http://scikit-learn.org/stable/\n",
    "\n",
    "Both pandas and scikit-learn are built on top of the core python numeric library `numpy`, so we'll start with a quick intro to `numpy`.\n",
    "\n",
    "https://numpy.org\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick intro to numpy\n",
    "`numpy` is the foundation on which most of python data analysis is built. It provides highly optimized routines for operating on multidimensional numeric data. The only things we really need to understand at this point are:\n",
    "* there are such things as `numpy` arrays (class `ndarray`)\n",
    "* we index into arrays like we index into lists, but with multiple indices if there are multiple dimensions\n",
    "* `numpy` has all sorts of fast numeric operations, so if we are dealing with lots of numeric data we are usually better off getting it into a numpy array than writing `for` loops ourselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how we import numpy\n",
    "import numpy as np\n",
    "\n",
    "# do this so that our plots show up in the notebook\n",
    "# may not be necessary in all environments\n",
    "#\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can create a numpy array from a list\n",
    "# here is a 1-dimensional array\n",
    "A1 = np.array( [0,4,5,6])\n",
    "print(A1)\n",
    "print('A1 has data type:',A1.dtype)\n",
    "print('A1 has shape:',A1.shape)\n",
    "\n",
    "print(A1[2]) # indexing into a 1-D array is like indexing into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also create a 2D array\n",
    "A2 = np.array( [ [0,3,4], [2,7,1.5] ])\n",
    "print('A2:',A2,sep='\\n')\n",
    "print('A2 has data type:',A2.dtype) # since we included a floating point number in the list\n",
    "print('A2 has shape:',A2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing into multidimensional arrays\n",
    "print('The element in row 0, column 1 is',A2[0,1])\n",
    "print('The element in row 1, column 2 is',A2[1,2])\n",
    "\n",
    "# we can take entire rows or columns by using : instead of a number\n",
    "print('Row 0 of A2 is', A2[0,:])\n",
    "print('Column 1 of A2 is', A2[:,1])\n",
    "\n",
    "# we don't have to take the entire row:\n",
    "print('The first two elements of row 1 of A1 are', A2[0,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# np.zeros is handy for creating and initializing an array:\n",
    "nrows = 4\n",
    "ncols = 5\n",
    "A = np.zeros((nrows, ncols))\n",
    "print(A)\n",
    "\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        A[i,j] = i*j\n",
    "print(A)\n",
    "\n",
    "# matplotlib is the core plotting library for python\n",
    "# pandas plotting and other fancy libraries like seaborn are built on top of matplotlib\n",
    "#\n",
    "import matplotlib.pyplot as plt # this is how we get easy access to all of matplotlib's plotting commands\n",
    "plt.imshow(A) # show the array A as a colored image\n",
    "plt.colorbar() # show the color scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy as lots of builtin functions that can be applied to arrays\n",
    "\n",
    "print('the max value in A is',A.max()) # max is a method of A\n",
    "print('again, the max value in A is',np.max(A)) # max is also a function in numpy\n",
    "print('the min value in A is',np.min(A)) \n",
    "print('the mean and standard deviation of the values in A are',np.mean(A), np.std(A)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use a builtin numpy routine to take the sine of the entire array A\n",
    "B = np.sin(A)\n",
    "plt.imshow(B)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 3 (rows) by 5 (columns) numpy array filled with random numbers between 0 and 1\n",
    "# use the random() method below to get the random numbers\n",
    "#\n",
    "# find the mean and standard deviation of the array values\n",
    "# find the mean of the 1st row (ie row index 0)\n",
    "# \n",
    "import random\n",
    "num = random.random()\n",
    "print('A single random number between 0 and 1, uniformly distributed:', num)\n",
    "\n",
    "# now create and fill the whole array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out what each of these four statements is doing. You could insert print() statements to \n",
    "# see what A looks like after each.\n",
    "\n",
    "A = np.zeros((10,2))\n",
    "\n",
    "A[:,0] = range(10)\n",
    "\n",
    "A[:,1] = np.cos(A[:,0])\n",
    "\n",
    "plt.plot(A[:,0], A[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On to `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how we import pandas\n",
    "#\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will typically get our data by reading from files. `pandas` has a variety of file-reading functions with names like `read_csv` and `read_excel` . Find them all by typing `pd.read_[TAB]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Fisher's famous iris dataset, which contains anatomical data on a variety of flowers\n",
    "# read_csv returns a pandas dataframe\n",
    "iris = pd.read_csv('data/iris_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first few rows\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has two main datatypes. A `DataFrame` like the `iris` object represents multiple columns of data (variables) for multiple rows of observations. A `Series` represents a single column or row of data.\n",
    "\n",
    "A DataFrame object has an `index` which represents and provides named access to the rows, and a `columns` attribute which represents the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This index was created automatically when the data was read in. It's just a counter.\n",
    "# Some datasets might have a natural index such as a date column, an observation identifier, etc.\n",
    "iris.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the columns of the iris dataset\n",
    "iris.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we get a single column, using dictionary-style indexing\n",
    "# we could also just write iris.sepal_length\n",
    "#\n",
    "iris['sepal_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a single column is of type Series\n",
    "type(iris['sepal_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super-useful function for getting summary statistics on a dataframe\n",
    "iris.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe` function just returns another `pandas DataFrame`, so we can use it for downstream analysis (more details on plotting below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new dataframe object with the results of iris.describe()\n",
    "df = iris.describe()\n",
    "print(df)\n",
    "\n",
    "# get rid of the \"count\" row because it screws up the y-scale on the bar plot\n",
    "# note that df.drop returns a copy, it does not modify df in-place\n",
    "df = df.drop('count')\n",
    "print(df)\n",
    "\n",
    "# now make a bar plot. Lots more on plotting later\n",
    "df.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for non-numeric columns we can count the number of times each value appears:\n",
    "df = iris['species'].value_counts()\n",
    "print(df)\n",
    "\n",
    "# this is kind of silly\n",
    "df.plot.bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to get the unique elements, by creating a python set object.\n",
    "#\n",
    "# Background: python sets are special-purpose containers that hold unordered sets of elements. \n",
    "# It is much faster to check if an element is in a set than to check if it's in a list. Creating\n",
    "# a set from a list or other kind of sequence data removes all the duplicates\n",
    "#\n",
    "set( iris['species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plotting with pandas\n",
    "Pandas has all sorts of built-in plotting routines. If `dataframe` is a pandas `DataFrame`, `dataframe.plot.TAB` will tab complete to a list of plotting functions.\n",
    "\n",
    "Pandas plotting is built on top of `matplotlib`, so plotting functions typically return `matplotlib` objects and `matplotlib.pyplot` commands can be used to tweak features of the plots just as when using plain-vanilla matplotlib. \n",
    "\n",
    "See the pandas visualization page:\n",
    "https://pandas.pydata.org/pandas-docs/stable/visualization.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe object has a couple plotting methods: `plot` and `hist` and maybe one or two others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many more plotting methods are available in `dataframe.plot.*`, which you can browse using tab completion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris.plot.<TAB>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot.hist(figsize=(8,6),alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe a histogram isn't the best visualization, how about kernel density estimation, which produces smoothed density plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot.density(figsize=(8,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots of one column against another are easy enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot.scatter( x='sepal_length', y='sepal_width', figsize=(8,6) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hexbin plot of petal_length versus petal_width\n",
    "# Try out different gridsizes\n",
    "# Compare the output to a scatterplot of petal_length versus petal_width\n",
    "# hexbin is one of the builtin plotting functions: iris.plot.hexbin?\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this plot represent? \n",
    "# You could first try typing iris.mean() to see what that dataframe looks like...\n",
    "\n",
    "iris.mean().plot.pie()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what about the `species` structure in this dataset?\n",
    "\n",
    "Let's add some color. This is super-easy to do using the plotting routines in the `seaborn` package, but for right now we'll stick with `pandas` and `matplotlib`.\n",
    "\n",
    "Definitely check out the [seaborn gallery](https://seaborn.pydata.org/examples/index.html), though.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are going to set up a dictionary that maps from iris species to plotting colors\n",
    "\n",
    "set_of_species = set( iris['species'] ) # get rid of duplicates in the species column\n",
    "\n",
    "color_dict = {}\n",
    "\n",
    "# zip is a useful builtin function that takes two lists/sets/sequences and \n",
    "#  lets you loop over them at the same time\n",
    "# Note how since we are looping over a pair of zipped lists we have two variables after for\n",
    "#\n",
    "# also note that I am sorting the set_of_species to get a consistent (alphabetical) ordering\n",
    "for species, color in zip( sorted(set_of_species), 'rgb' ):\n",
    "    print(species, color)\n",
    "    color_dict[ species ] = color\n",
    "    \n",
    "print(color_dict)\n",
    "\n",
    "# note that we don't have any control over which order the species come in:\n",
    "# compare the output if we add sorted(...) around set\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's use this dictionary to make a new column for the iris dataframe, containing the colors\n",
    "colors = []\n",
    "for species in iris['species']:\n",
    "    colors.append(color_dict[species])\n",
    "\n",
    "print(colors[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we add a column to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['color'] = colors #iris['species'].map(color_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can see the different flower species\n",
    "#\n",
    "iris.plot.scatter( x='sepal_length', y='sepal_width', c=iris['color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a nice way of comparing the values in different columns of a DataFrame\n",
    "pd.plotting.scatter_matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.plotting.scatter_matrix(iris,figsize=(12,12),diagonal='kde',c=iris['color']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another quick example w/ pandas: single-cell TCR sequencing data from 10x genomics \n",
    "This is the same dataset that was used for homework 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/vdj_v1_hs_pbmc_t_consensus_annotations.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter('reads','umis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter('length','umis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alas, this doesn't work because pandas gets mixed up about the boolean columns\n",
    "#pd.plotting.scatter_matrix(df,figsize=(12,12),diagonal='kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so instead we can make a new dataframe that just includes the numeric columns and plot \n",
    "# that\n",
    "\n",
    "df2 = df[ ['length', 'reads','umis']]\n",
    "\n",
    "pd.plotting.scatter_matrix(df2,figsize=(12,12),diagonal='kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with just the rows that have 'TRA' in the chain column\n",
    "# these are the alpha chains of the T cell receptor\n",
    "dfA = df[ df.chain=='TRA']\n",
    "\n",
    "# Create a dataframe with just the rows that have 'TRB' in the chain column\n",
    "dfB = df[ df.chain=='TRB']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(dfA[ ['length','reads','umis']],figsize=(12,12),diagonal='kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(dfB[ ['length','reads','umis']],figsize=(12,12),diagonal='kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redo the homework: look at how often each CDR3 nucleotide sequence occurs\n",
    "df['cdr3_nt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's easy to combine string columns.\n",
    "# Here we make a new column called tcr by concatenating some of the other info\n",
    "#\n",
    "df['tcr'] = df['v_gene']+'_'+df['j_gene']+'_'+df['cdr3']+'_'+df['cdr3_nt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now we can see more info about the recurring TCRs\n",
    "df['tcr'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning with the `scikit-learn` package\n",
    "\n",
    "borrowing heavily from \"The Python Data Science Handbook\" by Jake VanderPlas\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "\n",
    "## categories of machine learning\n",
    "Broadly speaking, machine learning for data science can be broken down into **supervised** and **unsupervised** methods. \n",
    "\n",
    "In **supervised** machine learning, we have a set of labeled training data, and we'd like to train a classifier or predictor in order to make predictions on some new set of testing data. We may be trying to predict something discrete, like different categories of data (for example, biological species from flower dimensions in the iris dataset; this is called *classification*), or predicting something continuous-valued like a response variable as a function of predictor variable(s) (this is called *regression*). \n",
    "\n",
    "In **unsupervised** machine learning, we are trying to discover hidden structure in a dataset. Two broad approaches are *clustering*, where we look for subgroups of similar data points, and *dimensionality reduction*, where we project a high-dimensional dataset into a lower-dimensional space while trying to preserve important features of the data points (e.g., keeping points that were nearby in the high-dimensional space also nearby in the low-dimensional projection)  \n",
    "\n",
    "`scikit-learn` has a nice flow-chart with even more information:\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "![logo](https://scikit-learn.org/stable/_static/ml_map.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised dimensionality reduction on the iris dataset with PCA\n",
    "\n",
    "`scikit-learn` has several methods for dimensionality reduction. One popular method is principal components analysis (PCA). \n",
    "\n",
    "Quoting from [the wikipedia entry for PCA](https://en.wikipedia.org/wiki/Principal_component_analysis), \"Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components\"\n",
    "\n",
    "This image shows a 2-D dataset with vectors indicating the directions of the first two principal components:\n",
    "\n",
    "![logo](https://upload.wikimedia.org/wikipedia/commons/f/f5/GaussianScatterPCA.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply PCA to the iris dataset, we need to create an array with just the numeric features we'll be using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a features array with just the measurement data\n",
    "\n",
    "X_iris = iris.drop(['species','color'], axis=1)\n",
    "X_iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iris.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A standard scikit-learn workflow\n",
    "\n",
    "### Step 1. create and configure the model (here `PCA`)\n",
    "\n",
    "### Step 2. `fit` the model to the data\n",
    "\n",
    "### Step 3. `transform` the data using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# This is the general scikit-learn workflow\n",
    "# 1. create and configure the model:\n",
    "pca = PCA(n_components=4)\n",
    "\n",
    "# 2. fit the model to the data\n",
    "pca.fit(X_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the four principal component vectors. They can be thought as vectors of weights, one weight for each of the columns in the original feature matrix, with the magnitude of each weight reflecting the contribution of that feature (e.g., petal_width) to the component. The first component gives the direction in feature space along which the greatest variation exists in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The principal components are all unit (length 1) vectors and they are \n",
    "#  orthogonal (perpendicular) to one another. Here we test that a bit.\n",
    "#\n",
    "\n",
    "print('The length of the first principal component is',np.linalg.norm(pca.components_[0]))\n",
    "print('The dot product of the first and second principal components is',\n",
    "    np.dot( pca.components_[0], pca.components_[3] ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The fraction of the total variance explained by each component is:', pca.explained_variance_ratio_)\n",
    "print('The total explained variance is :', np.sum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Now we can `transform` the data using the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Apply the model to 'transform' the data, in this case by mapping\n",
    "#    the input feature matrix onto the principal components\n",
    "#\n",
    "\n",
    "X_pca = pca.transform(X_iris)\n",
    "X_pca.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the first two columns of X_pca represent the axes of \n",
    "#  greatest variation, so we can plot just those and retain\n",
    "#  the maximum information.\n",
    "\n",
    "plt.scatter( X_pca[:,0], X_pca[:,1], c=colors )\n",
    "plt.title('First two principal components')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2'); # so notebook doesn't print the Text object returned by plt.ylabel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digression on rescaling the data\n",
    "\n",
    "This seems to work pretty well, but in some situations the multidimensional variables we are measuring are not directly comparable (for example because they have different units), in which case PCA will be dominated by the features that have the largest standard deviation. We can correct for this by first normalizing all columns to have mean 0 and standard deviation 1, using one of `scikit-learn`'s many preprocessing routines: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the standard deviations of the different features. \n",
    "#  They aren't too far apart (all within a factor of ~2)\n",
    "#\n",
    "X_iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new version of the data matrix in which each column\n",
    "#  has mean 0 and standard deviation 1.0 \n",
    "X_iris_scaled = scaler.fit_transform( X_iris )\n",
    "\n",
    "# check the mean, std-dev of the first column\n",
    "print( np.mean( X_iris_scaled[:,0]), np.std( X_iris_scaled[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2 = PCA(n_components=4)\n",
    "\n",
    "X_pca2 = pca2.fit_transform( X_iris_scaled )\n",
    "\n",
    "# set a larger figure size\n",
    "plt.figure(figsize=(14,8))\n",
    "\n",
    "# create a multi-panel plot:\n",
    "nrows=1 # one row,\n",
    "ncols=2 #  two columns\n",
    "\n",
    "plt.subplot(nrows, ncols, 1)\n",
    "plt.scatter( X_pca[:,0], X_pca[:,1], c=colors)\n",
    "plt.title('raw')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "\n",
    "plt.subplot(nrows, ncols, 2)\n",
    "plt.scatter( X_pca2[:,0], X_pca2[:,1], c=colors)\n",
    "plt.title('scaled')\n",
    "plt.xlabel('PCA1')\n",
    "\n",
    "\n",
    "print('old explained variance:',pca.explained_variance_ratio_)\n",
    "print('old components:',pca.components_)\n",
    "print('new explained variance:',pca2.explained_variance_ratio_)\n",
    "print('new components:',pca2.components_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised clustering on the iris dataset using K-means clustering\n",
    "\n",
    "K-means is an iterative clustering approach in which we repeatedly (1) choose K cluster centroids, (2) assign each data point to the closest centroid, and (3) update the centroids to reflect the means of the new clusters. The starting cluster centroids are chosen randomly (hence the use of `random_state=10` below to get reproducible results). Here's a snapshot from the [wikipedia page for K-means clustering]( https://en.wikipedia.org/wiki/K-means_clustering).\n",
    "\n",
    "![logo](kmeans.png)\n",
    "\n",
    "The naming convention I'm following here is to call the input (2-D) data `X` and the (1-D) classification/clusters `y` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# create the classifier object, tell it we are looking for 3 clusters\n",
    "#  and set the random_state for reproducibility\n",
    "kmeans = KMeans(n_clusters=3, random_state=10)\n",
    "\n",
    "# learn the cluster assignments\n",
    "kmeans.fit(X_iris)\n",
    "\n",
    "# create a new array with the cluster assignments, represented by the integers 0, 1, and 2.\n",
    "y_kmeans = kmeans.predict(X_iris)\n",
    "y_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the species/colors array to the kmeans cluster assignments\n",
    "np.array(colors) # converting to a numpy array gives a more compact view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we convert the kmeans clusters into colors so we can compare them with\n",
    "#  the species colors we set up earlier. Looking at the two arrays printed \n",
    "#  above, we can see a rough correspondence between kmeans cluster # and\n",
    "#  species color\n",
    "#\n",
    "#  cluster   color\n",
    "#    0         r\n",
    "#    1         g\n",
    "#    2         b\n",
    "\n",
    "kmeans_colors = []\n",
    "\n",
    "for cluster in y_kmeans:\n",
    "    if cluster==0:\n",
    "        kmeans_colors.append('r')\n",
    "    elif cluster==1:\n",
    "        kmeans_colors.append('g')\n",
    "    else:\n",
    "        assert cluster==2 # sanity check\n",
    "        kmeans_colors.append('b')\n",
    "\n",
    "    # or, more succinctly\n",
    "    #kmeans_colors.append('rgb'[cluster])\n",
    "    \n",
    "    \n",
    "error_colors = []\n",
    "for color, kmeans_color in zip( colors, kmeans_colors ):\n",
    "    if color == kmeans_color:\n",
    "        # correct cluster assignment\n",
    "        error_colors.append('g')\n",
    "    else:\n",
    "        # incorrect cluster assignment\n",
    "        error_colors.append('r')\n",
    "    \n",
    "\n",
    "# put three plots side-by-side\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter( X_pca[:,0], X_pca[:,1], c=colors)\n",
    "plt.title('species colors')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "# a more succinct way to set up kmeans_colors using a list comprehension\n",
    "#kmeans_colors = [ 'bgr'[x] for x in y_kmeans]\n",
    "plt.scatter( X_pca[:,0], X_pca[:,1], c=kmeans_colors)\n",
    "plt.title('kmeans cluster colors')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "# a more succinct way to set up error_colors using a list comprehension, zip, and conditional assignment\n",
    "#error_colors = [ ('red' if x!=y else 'green') for x,y in zip( colors, kmeans_colors) ]\n",
    "plt.scatter( X_pca[:,0], X_pca[:,1], c=error_colors, alpha=0.5)\n",
    "plt.title('species/cluster agreement\\ngreen=match, red=mismatch'); # semicolon so it doesn't print out Text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised classification on the iris dataset using Gaussian naive bayes classifier. \n",
    "\n",
    "A very simple approach to classification is to assume that the feature values for each class are drawn independently of one another from one dimensional gaussian distributions (bell-shaped curves), and to choose the predicted class labels that maximize the likelihood of the observed feature values. The Gaussian naive bayes classifier implements this method. Fitting the model involves estimating, for each feature (like sepal width) and each class (like 'setosa'), the mean and standard deviation of the feature distribution over the class members.\n",
    "\n",
    "For further details check out the [wikipedia page](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Gaussian_naive_Bayes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we need an array with the categories we are trying to classify:\n",
    "y_iris = iris['species']\n",
    "print(y_iris.head())\n",
    "y_iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose the model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "## create (and optionally configure) the model\n",
    "model = GaussianNB()\n",
    "\n",
    "## train the model\n",
    "model.fit(X_iris, y_iris)\n",
    "\n",
    "## see how well we do:\n",
    "y_model = model.predict(X_iris)\n",
    "y_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how well we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('The fraction of correctly classified samples is',accuracy_score(y_iris, y_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the problem with the above analysis?\n",
    "We trained and tested on the same set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## redo, correctly\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris,\n",
    "                                                random_state=30)\n",
    "\n",
    "# create the model\n",
    "model2 = GaussianNB()\n",
    "\n",
    "# train the model\n",
    "model2.fit(Xtrain, ytrain)\n",
    "\n",
    "# predict on NEW DATA that wasn't used for training\n",
    "y_model = model2.predict(Xtest)\n",
    "\n",
    "# assess accuracy\n",
    "accuracy_score(ytest, y_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using scikit-learn\n",
    "\n",
    "In linear regression we model a continuous-valued response variable as a linear combination of predictor variables. The parameters we need to fit are the coefficients for each predictor variable (the *slope* in the 1-D case), and if desired a constant offset (the *intercept*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by making some slightly noisy data using a linear function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = 2.\n",
    "intercept = -1\n",
    "\n",
    "noise_factor = 0.1\n",
    "\n",
    "num_points = 50\n",
    "\n",
    "# create uniformly distributed x-values between 0 and 1\n",
    "x = np.random.rand( num_points)\n",
    "print(x)\n",
    "\n",
    "# now create y values that are a linear function of the x-values, with some random noise added in\n",
    "# see how easy it is to multiply and add entire arrays of data with numpy\n",
    "# \n",
    "y = slope * x - intercept + noise_factor * np.random.randn(50)\n",
    "\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slightly tricky point is that scikit-learn routines generally expect a two-dimensional data array with one row for each observation (datapoint) and columns for each of the data values. Our set of x-values is actually a one-dimensional array (basically a list). So we need to change it's shape to make it two-dimensional, using the \"reshape\" command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"old 1D shape:\",x.shape)\n",
    "X = x.reshape( (num_points,1)) # note: upper-case X\n",
    "print(\"new 2D shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can fit the model to the data:\n",
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the slope value we recovered from the model: not too far from the actual value!\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and this is the intercept:\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the fitted line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a linear array of new test points\n",
    "xfit = np.linspace(0,1,10)\n",
    "\n",
    "# reshape into a 10-rows by 1-column 2D array to make scikit-learn happy\n",
    "Xfit = xfit.reshape((10,1))\n",
    "\n",
    "# predict the y-values using our fitted model:\n",
    "yfit = model.predict(Xfit)\n",
    "\n",
    "# scatter plot of original data \n",
    "plt.scatter(x, y)\n",
    "\n",
    "# line plot of fitted line\n",
    "plt.plot(xfit, yfit, c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random junk below here, probably unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another useful way of creating an array is to use the numpy random module\n",
    "# here we create an array of normally distributed values\n",
    "nrows = 10\n",
    "ncols = 10\n",
    "A = np.random.rand(nrows,ncols)\n",
    "print(A)\n",
    "plt.imshow(A)\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math # import the math module\n",
    "nrows = 1000\n",
    "ncols = 1000\n",
    "A = np.random.randn(nrows,ncols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "B = np.sin(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "B = np.zeros((nrows,ncols))\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        B[i,j] = math.sin( A[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this so the plots appear inline in the notebook:\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.sort_index(axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris.sort_values('petal_width',axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting: note that the default in most pandas operations is to return *copies* not to change the existing dataframe. This can usually be changed by passing inplace=True as one of the arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.groupby('species').mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_normed = X_iris.transform(lambda x: (x - x.mean()) / x.std())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
